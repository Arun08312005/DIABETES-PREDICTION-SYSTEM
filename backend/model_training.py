"""
Complete ML Pipeline for Diabetes Prediction Model
Simplified version for the project structure
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import joblib
import warnings
warnings.filterwarnings('ignore')

print("=" * 60)
print("DIABETES PREDICTION MODEL TRAINING PIPELINE")
print("=" * 60)

# 1. Load Data
print("\nğŸ“Š STEP 1: Loading Data...")
try:
    # Try local file first
    df = pd.read_csv('data/diabetes.csv')
    print(f"âœ… Data loaded from local file. Shape: {df.shape}")
except:
    print("âš ï¸  Local file not found, loading from URL...")
    url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv"
    df = pd.read_csv(url, header=None)
    df.columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
                  'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
    print(f"âœ… Data loaded from URL. Shape: {df.shape}")
    
    # Save to local file for future use
    df.to_csv('data/diabetes.csv', index=False)
    print("ğŸ’¾ Data saved to 'data/diabetes.csv'")

print("\nDataset Overview:")
print(df.head())
print(f"\nClass Distribution:")
print(df['Outcome'].value_counts())

# 2. Data Preprocessing
print("\nğŸ”„ STEP 2: Data Preprocessing...")

# Handle zeros in specific columns (they represent missing values)
zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in zero_columns:
    df[col] = df[col].replace(0, np.nan)
    df[col].fillna(df[col].median(), inplace=True)

print("âœ… Replaced zeros with median values")

# Separate features and target
X = df.drop('Outcome', axis=1)
y = df['Outcome']

print(f"\nFeature shape: {X.shape}")
print(f"Target shape: {y.shape}")

# 3. Split Data
print("\nğŸ“Š STEP 3: Splitting Data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ… Training set: {X_train.shape}")
print(f"âœ… Testing set: {X_test.shape}")

# 4. Feature Scaling
print("\nâš–ï¸ STEP 4: Feature Scaling...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("âœ… Features scaled using StandardScaler")

# Save the scaler
import os
os.makedirs('model', exist_ok=True)
joblib.dump(scaler, 'model/scaler.pkl')
print("ğŸ’¾ Scaler saved to 'model/scaler.pkl'")

# 5. Model Training
print("\nğŸ¤– STEP 5: Model Training...")

# Initialize and train Random Forest
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

print("Training Random Forest Classifier...")
model.fit(X_train_scaled, y_train)

# Cross-validation
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
print(f"âœ… Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# 6. Model Evaluation
print("\nğŸ“ˆ STEP 6: Model Evaluation...")

# Predictions
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# Calculate metrics
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)

print(f"ğŸ“Š Training Accuracy: {train_accuracy:.4f}")
print(f"ğŸ“Š Testing Accuracy: {test_accuracy:.4f}")
print(f"ğŸ“Š Precision: {precision:.4f}")
print(f"ğŸ“Š Recall: {recall:.4f}")
print(f"ğŸ“Š F1-Score: {f1:.4f}")

# Confusion Matrix
print("\nğŸ“‹ Confusion Matrix:")
cm = confusion_matrix(y_test, y_pred_test)
print(f"True Negatives: {cm[0][0]}")
print(f"False Positives: {cm[0][1]}")
print(f"False Negatives: {cm[1][0]}")
print(f"True Positives: {cm[1][1]}")

# Classification Report
print("\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred_test))

# 7. Feature Importance
print("\nğŸ” STEP 7: Feature Importance Analysis...")
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nFeature Importance Ranking:")
for i, (_, row) in enumerate(feature_importance.iterrows(), 1):
    print(f"{i:2d}. {row['Feature']:25s}: {row['Importance']:.4f}")

# 8. Save Model
print("\nğŸ’¾ STEP 8: Saving Model...")
joblib.dump(model, 'model/model.pkl')
print("âœ… Model saved to 'model/model.pkl'")

# 9. Final Summary
print("\n" + "=" * 60)
print("TRAINING PIPELINE COMPLETE")
print("=" * 60)
print(f"\nğŸ“ Files Created:")
print(f"  â€¢ model/model.pkl - Trained Random Forest model")
print(f"  â€¢ model/scaler.pkl - Feature scaler")
print(f"  â€¢ data/diabetes.csv - Dataset")
print(f"\nğŸ“Š Model Performance:")
print(f"  â€¢ Test Accuracy: {test_accuracy:.4f}")
print(f"  â€¢ Precision: {precision:.4f}")
print(f"  â€¢ Recall: {recall:.4f}")
print(f"  â€¢ F1-Score: {f1:.4f}")
print(f"\nğŸ¯ Top 3 Important Features:")
for i in range(3):
    print(f"  {i+1}. {feature_importance.iloc[i]['Feature']} ({feature_importance.iloc[i]['Importance']:.4f})")
print(f"\nâœ… Ready for deployment!")